{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41a0e24f"
   },
   "source": [
    "# Task\n",
    "Implement an AI-Based Disaster Detection and Rescue Support system using the C2A dataset. The system detect multiple natural disasters (floods, earthquakes, tsunamis, fires) and the number of people stuck, and based on this information, provide support team recommendations to enhance disaster response efficiency and support affected communities. The system quickly identifies disaster events and suggests appropriate emergency resources like rescue teams, medical aid, shelters, food, and evacuation plans. The goal is to enhance disaster response efficiency, reduce harm, and support affected communities in recovery through timely, tailored resource recommendations.\" The dataset is located at \"/content/drive/MyDrive/C2A.zip\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05cc24a7"
   },
   "source": [
    "## Setup environment \n",
    "\n",
    "### Subtask:\n",
    "Install necessary libraries and mount Google Drive to access the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8884,
     "status": "ok",
     "timestamp": 1754395789586,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "2a4f2da0",
    "outputId": "8aed5954-8462-450d-8950-a8684c307e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\admin\\miniconda3\\lib\\site-packages (8.3.174)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (3.10.5)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (1.16.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (2.7.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (2.3.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from ultralytics) (2.0.15)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\miniconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\miniconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "094232fb"
   },
   "source": [
    "## Load and extract dataset\n",
    "\n",
    "### Subtask:\n",
    "Load the zipped C2A dataset from Google Drive and extract its contents to a local directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29966,
     "status": "ok",
     "timestamp": 1754395822322,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "bf23b66b",
    "outputId": "5cad193d-4049-4516-d42e-8d46306deead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to: D:/path/to/c2a_dataset\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the corrected path to the zipped dataset in Google Drive\n",
    "zip_path = 'C:/Users/Admin/Downloads/archive (2).zip'\n",
    "\n",
    "# Define the path to the local directory where the dataset will be extracted\n",
    "extract_path = 'D:/path/to/c2a_dataset'\n",
    "\n",
    "# Create the extraction directory if it doesn't exist\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# Use the zipfile module to open and extract the contents of the zipped dataset\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Dataset extracted to: {extract_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e036b2bd"
   },
   "source": [
    "## Data preparation and preprocessing for human detection\n",
    "\n",
    "### Subtask:\n",
    "Define the classes for object detection (initially 'human'), organize the extracted image and annotation files, convert annotations to the YOLO format, split the dataset into training and validation sets, and prepare data loaders for the YOLO model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1840,
     "status": "ok",
     "timestamp": 1754395839505,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "e39652c4",
    "outputId": "706015fe-8d1c-4cd1-919d-9bbe89f91304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted annotations from D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/train/train_annotations.json to YOLO format in D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/train/labels\n",
      "Converted annotations from D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/val/val_annotations.json to YOLO format in D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/val/labels\n",
      "Annotations converted to YOLO format successfully.\n",
      "Train labels directory: D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/train/labels\n",
      "Validation labels directory: D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/val/labels\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def coco_to_yolo(coco_bbox, image_width, image_height):\n",
    "    \"\"\"Converts COCO bounding box [x_min, y_min, width, height] to YOLO format [x_center, y_center, width, height].\"\"\"\n",
    "    x_min, y_min, width, height = coco_bbox\n",
    "    x_center = (x_min + width / 2) / image_width\n",
    "    y_center = (y_min + height / 2) / image_height\n",
    "    w_norm = width / image_width\n",
    "    h_norm = height / image_height\n",
    "    return [x_center, y_center, w_norm, h_norm]\n",
    "\n",
    "def convert_annotations(json_path, image_dir, output_label_dir):\n",
    "    \"\"\"Converts COCO annotations in a JSON file to YOLO format .txt files.\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "\n",
    "    # Create a dictionary to map image_id to image information (including width and height)\n",
    "    image_info = {img['id']: (img['width'], img['height'], img['file_name']) for img in coco_data['images']}\n",
    "\n",
    "    # Create a dictionary to store annotations per image\n",
    "    annotations_by_image = {}\n",
    "    for ann in coco_data['annotations']:\n",
    "        image_id = ann['image_id']\n",
    "        if image_id not in annotations_by_image:\n",
    "            annotations_by_image[image_id] = []\n",
    "        annotations_by_image[image_id].append(ann)\n",
    "\n",
    "    # Process annotations for each image\n",
    "    for image_id, annotations in annotations_by_image.items():\n",
    "        img_width, img_height, file_name = image_info[image_id]\n",
    "        # YOLO label file name should match the image file name (but with .txt extension)\n",
    "        label_file_name = os.path.splitext(file_name)[0] + '.txt'\n",
    "        label_file_path = os.path.join(output_label_dir, label_file_name)\n",
    "\n",
    "        with open(label_file_path, 'w') as f:\n",
    "            for ann in annotations:\n",
    "                # Assuming 'human' is category_id 0 as observed in the sample annotation\n",
    "                category_id = ann['category_id'] # This should be 0 for 'human'\n",
    "                yolo_bbox = coco_to_yolo(ann['bbox'], img_width, img_height)\n",
    "                # Write the YOLO format: class_index center_x center_y width height\n",
    "                f.write(f\"{category_id} {yolo_bbox[0]:.6f} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f}\\n\")\n",
    "\n",
    "    print(f\"Converted annotations from {json_path} to YOLO format in {output_label_dir}\")\n",
    "\n",
    "# Define paths\n",
    "base_dataset_path = 'D:/path/to/c2a_dataset'\n",
    "train_json_path = os.path.join(r\"D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/train/train_annotations.json\")\n",
    "val_json_path = os.path.join(r\"D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/val/val_annotations.json\")\n",
    "\n",
    "train_image_dir = os.path.join(r\"D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/train/images\")\n",
    "val_image_dir = os.path.join(r\"D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/val/images\")\n",
    "\n",
    "train_label_dir = os.path.join(r\"D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/train/labels\")\n",
    "val_label_dir = os.path.join(r\"D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/val/labels\")\n",
    "\n",
    "# Convert train and validation annotations\n",
    "convert_annotations(train_json_path, train_image_dir, train_label_dir)\n",
    "convert_annotations(val_json_path, val_image_dir, val_label_dir)\n",
    "print(\"Annotations converted to YOLO format successfully.\")\n",
    "print(\"Train labels directory:\", train_label_dir)\n",
    "print(\"Validation labels directory:\", val_label_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1754395842356,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "09888580",
    "outputId": "508268c4-20bd-41fe-fce1-67027c18c05d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data.yaml file at: D:/path/to/c2a_dataset\\data.yaml\n",
      "\n",
      "Content of data.yaml:\n",
      "names: [human]\n",
      "nc: 1\n",
      "train: D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/train/images\n",
      "val: D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/val/images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create the `data.yaml` configuration file required by YOLO, specifying the paths to the training and validation image and label directories,\n",
    "#the number of classes, and the class names.\n",
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Define the content of the data.yaml file\n",
    "data_yaml_content = {\n",
    "    'train': os.path.join(r\"D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/train/images\"), # Path to training images\n",
    "    'val': os.path.join(r\"D:/path/to/c2a_dataset/C2A_Dataset/new_dataset3/val/images\"),     # Path to validation images\n",
    "    'nc': 1,                                                     # Number of classes\n",
    "    'names': ['human']                                           # Class names\n",
    "}\n",
    "\n",
    "# Define the path where the data.yaml file will be saved\n",
    "data_yaml_path = os.path.join(base_dataset_path, 'data.yaml')\n",
    "\n",
    "# Write the content to the data.yaml file\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_yaml_content, f, default_flow_style=None)\n",
    "\n",
    "print(f\"Created data.yaml file at: {data_yaml_path}\")\n",
    "\n",
    "# Display the content of the created data.yaml file\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    print(\"\\nContent of data.yaml:\")\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e6d48e8"
   },
   "source": [
    "## Model building and Training: human detection\n",
    "\n",
    "### Subtask:\n",
    " Add YOLO model architecture (e.g., YOLOv8) and initialize the YOLO model, potentially using pre-trained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2441025,
     "status": "ok",
     "timestamp": 1754398289532,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "fe5373c8",
    "outputId": "47341feb-c6b3-4f7f-c9c4-06f67e1b4754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.175 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.174  Python-3.13.5 torch-2.7.1+cpu CPU (Intel Core(TM) i5-8365U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=D:/path/to/c2a_dataset\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train7, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train7, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 300.481.6 MB/s, size: 3085.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\path\\to\\c2a_dataset\\C2A_Dataset\\new_dataset3\\train\\labels.cache... 6129 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6129/6129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\path\\to\\c2a_dataset\\C2A_Dataset\\new_dataset3\\train\\images\\flood_image0407_3.png: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 193.3108.5 MB/s, size: 321.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\path\\to\\c2a_dataset\\C2A_Dataset\\new_dataset3\\val\\labels.cache... 2043 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2043/2043 [00:00<?, ?it/s]\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train7\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train7\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.676      1.478      1.071        649        640: 100%|██████████| 192/192 [58:23<00:00, 18.25s/it] \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [08:20<00:00, 15.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2043      72123      0.717      0.552      0.594      0.317\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.515      1.028      1.025        607        640: 100%|██████████| 192/192 [51:00<00:00, 15.94s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [07:24<00:00, 13.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2043      72123      0.701      0.577      0.608       0.33\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.469     0.9645      1.014        817        640: 100%|██████████| 192/192 [50:34<00:00, 15.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [06:58<00:00, 13.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2043      72123      0.743        0.6      0.647       0.37\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.422     0.9192      1.003        852        640: 100%|██████████| 192/192 [51:26<00:00, 16.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [06:54<00:00, 12.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2043      72123      0.742      0.614      0.655       0.37\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.377     0.8799     0.9906        739        640: 100%|██████████| 192/192 [51:21<00:00, 16.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [06:55<00:00, 13.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2043      72123      0.767      0.635      0.685        0.4\n",
      "\n",
      "5 epochs completed in 4.991 hours.\n",
      "Optimizer stripped from runs\\detect\\train7\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train7\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train7\\weights\\best.pt...\n",
      "Ultralytics 8.3.174  Python-3.13.5 torch-2.7.1+cpu CPU (Intel Core(TM) i5-8365U 1.60GHz)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [04:48<00:00,  9.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2043      72123      0.767      0.634      0.685        0.4\n",
      "Speed: 2.3ms preprocess, 111.3ms inference, 0.0ms loss, 9.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Choose a YOLOv8 model architecture (e.g., 'yolov8n.pt' for nano model with pre-trained weights)\n",
    "model_architecture = 'yolov8n.pt'\n",
    "\n",
    "# Instantiate the YOLO model\n",
    "model = YOLO(model_architecture)\n",
    "\n",
    "# Print the model to inspect its layers\n",
    "#print(model)\n",
    "results = model.train(data=data_yaml_path, epochs=5, batch=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKTJza-stR_3"
   },
   "source": [
    "#Model Evaluation : Human detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "error",
     "timestamp": 1754407403376,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "h_DO8djRs6rk",
    "outputId": "0a5eddf5-3efd-4580-b543-71efec4991a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Human Detection Model...\n",
      "Ultralytics 8.3.174  Python-3.13.5 torch-2.7.1+cpu CPU (Intel Core(TM) i5-8365U 1.60GHz)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1145.8302.6 MB/s, size: 600.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\path\\to\\c2a_dataset\\C2A_Dataset\\new_dataset3\\val\\labels.cache... 2043 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2043/2043 [00:00<?, ?it/s]\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 64/64 [04:28<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2043      72123      0.767      0.634      0.685        0.4\n",
      "Speed: 1.6ms preprocess, 99.1ms inference, 0.0ms loss, 6.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train72\u001b[0m\n",
      "\n",
      " Human Detection Model Evaluation Complete!\n",
      " YOLO Evaluation Metrics:\n",
      "  mAP@0.5: 0.6854\n",
      "  mAP@0.5:0.95: 0.3999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"Evaluating Human Detection Model...\")\n",
    "\n",
    "yolo_results = model.val(data=data_yaml_path) \n",
    "\n",
    "# Print YOLO evaluation metrics\n",
    "print(\"\\n Human Detection Model Evaluation Complete!\")\n",
    "print(\" YOLO Evaluation Metrics:\")\n",
    "mAP50 = yolo_results.results_dict.get('metrics/mAP50(B)', 'N/A')\n",
    "mAP50_95 = yolo_results.results_dict.get('metrics/mAP50-95(B)', 'N/A')\n",
    "\n",
    "\n",
    "print(f\"  mAP@0.5: {mAP50:.4f}\")\n",
    "print(f\"  mAP@0.5:0.95: {mAP50_95:.4f}\")\n",
    "\n",
    "# Plotting the evaluation metrics\n",
    "metrics = ['mAP@0.5', 'mAP@0.5:0.95']\n",
    "values = [mAP50, mAP50_95]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics, values, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('YOLO Human Detection Model Evaluation Metrics')\n",
    "plt.ylim(0, 1) # Metrics are typically in the range [0, 1]\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a68183c"
   },
   "source": [
    "## Data preparation and preprocessing for disaster type classification\n",
    "\n",
    "### Subtask:\n",
    "Infer disaster types from image filenames or directory structure, define the classes for image classification based on inferred types, organize images into class-specific subfolders for training and validation, and prepare data loaders for the image classification model, including necessary preprocessing steps like resizing and normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2108,
     "status": "ok",
     "timestamp": 1754398331770,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "3e4b0d5b",
    "outputId": "10b3786d-eb8f-476f-cc39-399f10be92b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using disaster types for classification: ['collapsed_building', 'flood', 'fire', 'traffic_incident']\n",
      "Found 6129 images belonging to 4 classes.\n",
      "Found 2043 images belonging to 4 classes.\n",
      "\n",
      "Training and validation data generators created.\n",
      "Training classes: {'collapsed_building': 0, 'flood': 1, 'fire': 2, 'traffic_incident': 3}\n",
      "Validation classes: {'collapsed_building': 0, 'flood': 1, 'fire': 2, 'traffic_incident': 3}\n",
      "Batch 1: images shape (32, 224, 224, 3), labels shape (32, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define base paths\n",
    "base_dataset_path = r'D:\\c2a_dataset\\C2A_Dataset\\new_dataset3'\n",
    "train_base_path = os.path.join(base_dataset_path, 'train')\n",
    "val_base_path = os.path.join(base_dataset_path, 'val')\n",
    "\n",
    "# Define disaster types (used for fixed class order in generators)\n",
    "disaster_types = ['collapsed_building', 'flood', 'fire', 'traffic_incident']\n",
    "print(f\"Using disaster types for classification: {disaster_types}\")\n",
    "\n",
    "# -- IMAGE DATA GENERATORS (DATA LOADERS) --\n",
    "\n",
    "# Define image preprocessing and augmentation for train/validation\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# These generators will index folders exactly as disaster_types are listed\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_base_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    classes=disaster_types\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_base_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    classes=disaster_types\n",
    ")\n",
    "\n",
    "print(\"\\nTraining and validation data generators created.\")\n",
    "print(f\"Training classes: {train_generator.class_indices}\")\n",
    "print(f\"Validation classes: {val_generator.class_indices}\")\n",
    "\n",
    "# Function to preview a few samples from the data generator\n",
    "def preview_samples(data_generator, n_batches=1):\n",
    "    for i, (images, labels) in enumerate(data_generator):\n",
    "        print(f\"Batch {i+1}: images shape {images.shape}, labels shape {labels.shape}\")\n",
    "        if i+1 >= n_batches:\n",
    "            break\n",
    "\n",
    "# Preview a single batch of training data \n",
    "preview_samples(train_generator, n_batches=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b037310"
   },
   "source": [
    "## Model building and Training: disaster type classification\n",
    "\n",
    "### Subtask:\n",
    "Choose a suitable image classification model architecture (e.g., a pre-trained model like ResNet or EfficientNet for transfer learning), load the pre-trained weights, and modify the final layers for the specific number of disaster classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319193,
     "status": "ok",
     "timestamp": 1754404807925,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "lgvLNe46SFjw",
    "outputId": "81f94b21-be05-4b1a-b2f5-eee9aef304eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - Precision: 0.8171 - Recall: 0.6722 - accuracy: 0.7414 - loss: 0.6273"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 3s/step - Precision: 0.9186 - Recall: 0.8319 - accuracy: 0.8706 - loss: 0.3522 - val_Precision: 0.9631 - val_Recall: 0.9334 - val_accuracy: 0.9496 - val_loss: 0.1625\n",
      "Epoch 2/5\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 2s/step - Precision: 0.9730 - Recall: 0.9522 - accuracy: 0.9635 - loss: 0.1284 - val_Precision: 0.9767 - val_Recall: 0.9623 - val_accuracy: 0.9697 - val_loss: 0.1056\n",
      "Epoch 3/5\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 2s/step - Precision: 0.9847 - Recall: 0.9745 - accuracy: 0.9791 - loss: 0.0830 - val_Precision: 0.9827 - val_Recall: 0.9750 - val_accuracy: 0.9785 - val_loss: 0.0812\n",
      "Epoch 4/5\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 2s/step - Precision: 0.9903 - Recall: 0.9830 - accuracy: 0.9873 - loss: 0.0586 - val_Precision: 0.9848 - val_Recall: 0.9804 - val_accuracy: 0.9834 - val_loss: 0.0672\n",
      "Epoch 5/5\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 2s/step - Precision: 0.9962 - Recall: 0.9928 - accuracy: 0.9949 - loss: 0.0441 - val_Precision: 0.9863 - val_Recall: 0.9843 - val_accuracy: 0.9843 - val_loss: 0.0590\n",
      "\n",
      "Image classification model training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the model before compiling and training\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "\n",
    "# Define the base pre-trained model\n",
    "base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new Keras Functional API model on top of the frozen base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a final dense layer with num_classes units and softmax activation\n",
    "num_classes = len(disaster_types)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model_classification = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model_classification.compile(optimizer='adam',\n",
    "                             loss='categorical_crossentropy',\n",
    "                             metrics=['accuracy', 'Precision', 'Recall', ])\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "history = model_classification.fit(\n",
    "    train_generator,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "print(\"\\nImage classification model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f2321e2"
   },
   "source": [
    "## Model Evaluation:disaster type classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325627,
     "status": "ok",
     "timestamp": 1754405623832,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "86bcfbae",
    "outputId": "651ae25b-f548-49ad-a373-f2ab895de8b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Precision', 'Recall', 'accuracy', 'loss', 'val_Precision', 'val_Recall', 'val_accuracy', 'val_loss'])\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - Precision: 0.9862 - Recall: 0.9824 - accuracy: 0.9848 - loss: 0.0550\n",
      "\n",
      " Disaster Type Classification Model Evaluation Complete!\n",
      " Classification Evaluation Metrics:\n",
      "  Validation Loss: 0.0550\n",
      "  Validation Accuracy: 0.9848\n",
      "  Validation Precision: 0.986240804195404\n",
      "  Validation Recall: 0.9823788404464722\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss, accuracy = model_classification.evaluate(val_generator)\n",
    "\n",
    "# Print classification evaluation metrics\n",
    "print(\"\\n Disaster Type Classification Model Evaluation Complete!\")\n",
    "print(\" Classification Evaluation Metrics:\")\n",
    "# Plotting the evaluation metrics\n",
    "metrics = ['Validation Loss', 'Validation Accuracy']\n",
    "values = [loss, accuracy]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics, values, color=['salmon', 'cornflowerblue'])\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Disaster Type Classification Model Evaluation Metrics')\n",
    "plt.ylim(0, max(loss, accuracy) + 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4279795"
   },
   "source": [
    "## Model Integration and Resource Recommendation Logic\n",
    "\n",
    "### Subtask:\n",
    "Integrate the two trained models (human detection and disaster type classification). Develop logic to analyze an image using both models and recommend appropriate emergency resources based on the detected human count and the classified disaster type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "14d649a0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_preprocess_input \n",
    "import base64 \n",
    "RESCUE_RESOURCES = {\n",
    "    'collapsed_building': {\n",
    "        'no_humans': ['Search and rescue dogs', 'Structural assessment team', 'Heavy equipment'],\n",
    "        'low_density': ['Ground rescue teams (small)', 'Medical aid (basic)', 'Search and rescue dogs'],\n",
    "        'medium_density': ['Ground rescue teams (multiple)', 'Medical aid (advanced)', 'Temporary shelter assessment'],\n",
    "        'high_density': ['Multiple rescue teams (heavy urban search and rescue)', 'Medical triage setup', 'Temporary shelters', 'Food and water supplies']\n",
    "    },\n",
    "    'flood': {\n",
    "        'no_humans': ['Flood damage assessment', 'Infrastructure check'],\n",
    "        'low_density': ['Water rescue team (small boat)', 'Medical aid (basic)', 'Evacuation route assessment'],\n",
    "        'medium_density': ['Water rescue teams (multiple boats)', 'Medical aid (advanced)', 'Evacuation support', 'Temporary shelters'],\n",
    "        'high_density': ['Multiple water rescue teams (boats/helicopters)', 'Medical triage setup', 'Temporary shelters', 'Food and water supplies', 'Evacuation plans']\n",
    "    },\n",
    "    'fire': {\n",
    "        'no_humans': ['Fire damage assessment', 'Safety perimeter setup'],\n",
    "        'low_density': ['Firefighting units (local)', 'Medical aid (basic)'],\n",
    "        'medium_density': ['Firefighting units (multiple)', 'Medical aid (advanced)', 'Evacuation route assessment'],\n",
    "        'high_density': ['Multiple firefighting units (regional support)', 'Medical triage setup', 'Temporary shelters', 'Evacuation plans']\n",
    "    },\n",
    "    'traffic_incident': {\n",
    "        'no_humans': ['Traffic management', 'Scene cleanup'],\n",
    "        'low_density': ['Police/Emergency medical (basic)', 'Tow trucks'],\n",
    "        'medium_density': ['Multiple police/emergency medical', 'Traffic management', 'Accident investigation'],\n",
    "        'high_density': ['Major incident response team', 'Medical triage setup', 'Heavy recovery equipment']\n",
    "    },\n",
    "     'other': { # Default for uncategorized or unknown disaster types\n",
    "        'no_humans': ['General assessment team'],\n",
    "        'low_density': ['Basic rescue team', 'Medical aid (basic)'],\n",
    "        'medium_density': ['Multiple rescue teams', 'Medical aid (advanced)'],\n",
    "        'high_density': ['Major rescue operations', 'Medical triage setup']\n",
    "    }\n",
    "}\n",
    "\n",
    "class IntegratedDisasterSystem:\n",
    "    def __init__(self, yolo_model, classification_model, classification_class_names):\n",
    "        self.yolo_model = yolo_model\n",
    "        self.classification_model = classification_model\n",
    "        self.classification_class_names = classification_class_names\n",
    "        print(\"Integrated Disaster System Initialized\")\n",
    "\n",
    "    def classify_disaster_type(self, image_path):\n",
    "        \"\"\"Classify the type of disaster in the image\"\"\"\n",
    "        img = keras_image.load_img(image_path, target_size=(self.classification_model.input_shape[1], self.classification_model.input_shape[2]))\n",
    "        img_array = keras_image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        # Preprocess the image for ResNet\n",
    "        img_array = resnet_preprocess_input(img_array)\n",
    "\n",
    "        predictions = self.classification_model.predict(img_array, verbose=0)\n",
    "        predicted_class_index = np.argmax(predictions[0])\n",
    "        predicted_class_name = self.classification_class_names[predicted_class_index]\n",
    "        confidence = np.max(predictions[0])\n",
    "\n",
    "        return predicted_class_name, float(confidence)\n",
    "\n",
    "\n",
    "    def detect_humans(self, image_path):\n",
    "        \"\"\"Detect humans in disaster scene using YOLO model\"\"\"\n",
    "\n",
    "        results = self.yolo_model(image_path, conf=0.25, verbose=False)\n",
    "\n",
    "        detections = []\n",
    "        human_count = 0\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                        # 'human' is class 0 in the YOLO model\n",
    "                    if box.cls[0] == 0:\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        conf = box.conf[0].cpu().numpy()\n",
    "                        detections.append({\n",
    "                                'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                                'confidence': float(conf),\n",
    "                                'center': [int((x1+x2)/2), int((y1+y2)/2)]\n",
    "                            })\n",
    "                        human_count += 1\n",
    "\n",
    "        return detections, human_count\n",
    "\n",
    "\n",
    "    def assess_human_density(self, human_count):\n",
    "        \"\"\"Assess human density category\"\"\"\n",
    "        if human_count == 0:\n",
    "            return 'no_humans'\n",
    "        elif human_count <= 2:\n",
    "            return 'low_density'\n",
    "        elif human_count <= 5:\n",
    "            return 'medium_density'\n",
    "        else:\n",
    "            return 'high_density'\n",
    "\n",
    "    def recommend_resources(self, disaster_type, human_density_category):\n",
    "        \"\"\"Recommend resources based on disaster type and human density\"\"\"\n",
    "        disaster_key = disaster_type.lower() if disaster_type.lower() in RESCUE_RESOURCES else 'other'\n",
    "        density_key = human_density_category.lower() if human_density_category.lower() in RESCUE_RESOURCES[disaster_key] else 'no_humans'\n",
    "\n",
    "        return RESCUE_RESOURCES[disaster_key][density_key]\n",
    "\n",
    "\n",
    "    def analyze_scene(self, image_path):\n",
    "        \"\"\"Analyze a single disaster scene and provide recommendations\"\"\"\n",
    "        print(f\"\\n--- Analyzing Scene: {os.path.basename(image_path)} ---\")\n",
    "\n",
    "        # 1. Classify Disaster Type\n",
    "        disaster_type, type_confidence = self.classify_disaster_type(image_path)\n",
    "        print(f\" Classified Disaster Type: {disaster_type.upper()} (Confidence: {type_confidence:.2f})\")\n",
    "\n",
    "        # 2. Detect Humans\n",
    "        detections, human_count = self.detect_humans(image_path)\n",
    "        print(f\"Humans Detected: {human_count}\")\n",
    "        print(f\"Human Detections Details: {detections}\") \n",
    "\n",
    "        # 3. Assess Human Density\n",
    "        human_density_category = self.assess_human_density(human_count)\n",
    "        print(f\" Human Density Category: {human_density_category.replace('_', ' ').title()}\")\n",
    "\n",
    "        # 4. Recommend Resources\n",
    "        recommended_resources = self.recommend_resources(disaster_type, human_density_category)\n",
    "        print(f\"Recommended Resources:\")\n",
    "        for i, resource in enumerate(recommended_resources, 1):\n",
    "            print(f\"   {i}. {resource}\")\n",
    "        \n",
    "        # 5. image in the report (as base64)\n",
    "        image_base64 = None\n",
    "        try:\n",
    "            with open(image_path, \"rb\") as img_file:\n",
    "                image_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding image to base64: {e}\")\n",
    "\n",
    "        # Return a structured report\n",
    "        report = {\n",
    "            'image_path': image_path,  \n",
    "            'disaster_classification': {'type': disaster_type, 'confidence': type_confidence},\n",
    "            'human_detection': {'count': human_count, 'detections': detections},\n",
    "            'human_density_category': human_density_category,\n",
    "            'recommended_resources': recommended_resources,\n",
    "            'image_base64': image_base64  \n",
    "        }\n",
    "        return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d521d6dd"
   },
   "source": [
    "## Analysis and Visualization Functions\n",
    "\n",
    "### Subtask:\n",
    "Create functions to analyze a given image using the integrated system, generate a report, and visualize the results (e.g., bounding boxes for humans, overlaid disaster type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 849
    },
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1754408068088,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "6f6b464d",
    "outputId": "48a9a296-041c-4687-cf5c-fe92c0938ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2043 images in the test folder.\n",
      "Image files: ['collapsed_building_image0001_3.png', 'collapsed_building_image0002_1.png', 'collapsed_building_image0002_3.png', 'collapsed_building_image0004_1.png', 'collapsed_building_image0005_2.png']...\n",
      "Selected random image: D:/c2a_dataset/C2A_Dataset/new_dataset3/test/images/collapsed_building_image0139_1.png\n",
      "Integrated Disaster System Initialized\n",
      "\n",
      "--- Analyzing Scene: collapsed_building_image0139_1.png ---\n",
      " Classified Disaster Type: COLLAPSED_BUILDING (Confidence: 0.97)\n",
      "Humans Detected: 21\n",
      "Human Detections Details: [{'bbox': [349, 66, 376, 86], 'confidence': 0.8836973905563354, 'center': [362, 76]}, {'bbox': [320, 252, 351, 268], 'confidence': 0.8163034915924072, 'center': [335, 260]}, {'bbox': [331, 30, 340, 53], 'confidence': 0.8050060868263245, 'center': [336, 41]}, {'bbox': [59, 53, 74, 84], 'confidence': 0.8020620346069336, 'center': [66, 69]}, {'bbox': [248, 106, 266, 119], 'confidence': 0.7907145023345947, 'center': [257, 112]}, {'bbox': [119, 301, 150, 317], 'confidence': 0.7671607732772827, 'center': [135, 309]}, {'bbox': [274, 9, 285, 23], 'confidence': 0.7431891560554504, 'center': [280, 16]}, {'bbox': [142, 13, 147, 23], 'confidence': 0.6816532015800476, 'center': [144, 18]}, {'bbox': [30, 65, 54, 84], 'confidence': 0.6773988604545593, 'center': [42, 75]}, {'bbox': [89, 182, 98, 201], 'confidence': 0.6156773567199707, 'center': [94, 192]}, {'bbox': [104, 262, 114, 284], 'confidence': 0.6010198593139648, 'center': [109, 273]}, {'bbox': [272, 0, 281, 7], 'confidence': 0.5795127153396606, 'center': [276, 4]}, {'bbox': [139, 269, 151, 279], 'confidence': 0.5322035551071167, 'center': [145, 274]}, {'bbox': [125, 228, 135, 237], 'confidence': 0.528907299041748, 'center': [130, 232]}, {'bbox': [361, 250, 375, 260], 'confidence': 0.5128161907196045, 'center': [368, 255]}, {'bbox': [296, 193, 303, 207], 'confidence': 0.45390406250953674, 'center': [299, 200]}, {'bbox': [209, 299, 217, 303], 'confidence': 0.4160248935222626, 'center': [213, 301]}, {'bbox': [317, 211, 323, 222], 'confidence': 0.3908746838569641, 'center': [320, 216]}, {'bbox': [152, 290, 156, 296], 'confidence': 0.3508611023426056, 'center': [154, 293]}, {'bbox': [208, 298, 218, 305], 'confidence': 0.29740071296691895, 'center': [213, 302]}, {'bbox': [317, 210, 356, 223], 'confidence': 0.2815180718898773, 'center': [336, 216]}]\n",
      " Human Density Category: High Density\n",
      "Recommended Resources:\n",
      "   1. Multiple rescue teams (heavy urban search and rescue)\n",
      "   2. Medical triage setup\n",
      "   3. Temporary shelters\n",
      "   4. Food and water supplies\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_disaster_analysis(image_path, analysis_report, save_path=None):\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\" Error loading image for visualization: {image_path}\")\n",
    "        return\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = img_rgb.shape\n",
    "\n",
    "    # Draw bounding boxes for detected humans\n",
    "    detections = analysis_report.get('human_detection', {}).get('detections', [])\n",
    "    for det in detections:\n",
    "        bbox = det['bbox']\n",
    "        confidence = det['confidence']\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # Ensure coordinates are within image bounds\n",
    "        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(width, x2), min(height, y2)\n",
    "        cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2) # Green bounding box\n",
    "        label = f\"Human: {confidence:.2f}\"\n",
    "        cv2.putText(img_rgb, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    " \n",
    "# ------- Random Image Selection and Visualization -------\n",
    "\n",
    "# Path to the test images folder\n",
    "test_images_folder = r'D:/c2a_dataset/C2A_Dataset/new_dataset3/test/images'\n",
    "\n",
    "# List image files (case-insensitive match on common image formats)\n",
    "image_files = [f for f in os.listdir(test_images_folder)\n",
    "               if f.lower().endswith('.png')]\n",
    "print(f\"Found {len(image_files)} images in the test folder.\")\n",
    "print(f\"Image files: {image_files[:5]}...\")  # Display first 5 files for brevity\n",
    "\n",
    "if not image_files:\n",
    "    print(\"No images found in the specified test images folder!\")\n",
    "else:\n",
    "    # Pick a random image\n",
    "    random_image_file = random.choice(image_files)\n",
    "    random_image_path = os.path.join(test_images_folder, random_image_file)\n",
    "    random_image_path = random_image_path.replace(\"\\\\\", \"/\")\n",
    "    image_path = random_image_path\n",
    "    print(f\"Selected random image: {image_path}\")\n",
    "\n",
    "# Function to visualize the disaster analysis report\n",
    "    integrated_system = IntegratedDisasterSystem(yolo_model = model, classification_model = model_classification, classification_class_names = disaster_types)\n",
    "\n",
    "    analysis_report = integrated_system.analyze_scene(image_path) \n",
    "    visualize_disaster_analysis(random_image_path, analysis_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34d9428e"
   },
   "source": [
    "## System Demo\n",
    "\n",
    "### Subtask:\n",
    "Develop a demo to showcase the complete system's capabilities using sample disaster images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1754409039210,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "49ea5511",
    "outputId": "c41e115b-f932-40af-82be-abb89f00da91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PROCESSING DISASTER AREA: D:/c2a_dataset/C2A_Dataset/new_dataset3/test/images\n",
      "============================================================\n",
      "Processing 5 randomly selected images as requested.\n",
      "Found 5 images to process.\n",
      "\n",
      "--- Processing Image 1/5: flood_image0319_3.png ---\n",
      "\n",
      "--- Analyzing Scene: flood_image0319_3.png ---\n",
      " Classified Disaster Type: FLOOD (Confidence: 1.00)\n",
      "Humans Detected: 27\n",
      "Human Detections Details: [{'bbox': [403, 350, 429, 375], 'confidence': 0.8069194555282593, 'center': [416, 363]}, {'bbox': [606, 173, 630, 191], 'confidence': 0.7849034667015076, 'center': [618, 182]}, {'bbox': [314, 572, 339, 591], 'confidence': 0.7820339202880859, 'center': [326, 582]}, {'bbox': [300, 255, 324, 275], 'confidence': 0.7300440669059753, 'center': [312, 265]}, {'bbox': [302, 702, 324, 727], 'confidence': 0.7281609773635864, 'center': [313, 714]}, {'bbox': [653, 578, 668, 603], 'confidence': 0.6974872946739197, 'center': [660, 591]}, {'bbox': [911, 98, 920, 116], 'confidence': 0.616899311542511, 'center': [916, 107]}, {'bbox': [597, 58, 623, 79], 'confidence': 0.616185188293457, 'center': [610, 68]}, {'bbox': [658, 420, 671, 453], 'confidence': 0.6099444627761841, 'center': [664, 437]}, {'bbox': [406, 712, 421, 740], 'confidence': 0.5597003102302551, 'center': [414, 726]}, {'bbox': [381, 482, 397, 508], 'confidence': 0.5311280488967896, 'center': [389, 495]}, {'bbox': [974, 159, 983, 168], 'confidence': 0.506820797920227, 'center': [979, 163]}, {'bbox': [912, 720, 924, 742], 'confidence': 0.48373398184776306, 'center': [918, 731]}, {'bbox': [831, 568, 854, 604], 'confidence': 0.46114015579223633, 'center': [843, 586]}, {'bbox': [856, 42, 901, 55], 'confidence': 0.4540795683860779, 'center': [879, 49]}, {'bbox': [762, 686, 777, 704], 'confidence': 0.43648117780685425, 'center': [770, 695]}, {'bbox': [737, 591, 751, 606], 'confidence': 0.4074373245239258, 'center': [744, 599]}, {'bbox': [679, 610, 696, 622], 'confidence': 0.3994767367839813, 'center': [687, 616]}, {'bbox': [352, 468, 362, 485], 'confidence': 0.393390953540802, 'center': [357, 477]}, {'bbox': [126, 746, 134, 760], 'confidence': 0.3752574324607849, 'center': [130, 753]}, {'bbox': [545, 600, 552, 616], 'confidence': 0.3176426291465759, 'center': [549, 608]}, {'bbox': [649, 674, 655, 686], 'confidence': 0.2994750142097473, 'center': [652, 680]}, {'bbox': [604, 4, 623, 18], 'confidence': 0.2870469093322754, 'center': [613, 11]}, {'bbox': [894, 567, 908, 596], 'confidence': 0.2848149836063385, 'center': [901, 582]}, {'bbox': [608, 625, 617, 635], 'confidence': 0.2822858691215515, 'center': [613, 630]}, {'bbox': [583, 308, 595, 321], 'confidence': 0.26886841654777527, 'center': [589, 315]}, {'bbox': [489, 261, 500, 274], 'confidence': 0.257348895072937, 'center': [495, 268]}]\n",
      " Human Density Category: High Density\n",
      "Recommended Resources:\n",
      "   1. Multiple water rescue teams (boats/helicopters)\n",
      "   2. Medical triage setup\n",
      "   3. Temporary shelters\n",
      "   4. Food and water supplies\n",
      "   5. Evacuation plans\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Image 2/5: traffic_incident_image0446_2.png ---\n",
      "\n",
      "--- Analyzing Scene: traffic_incident_image0446_2.png ---\n",
      " Classified Disaster Type: TRAFFIC_INCIDENT (Confidence: 0.86)\n",
      "Humans Detected: 44\n",
      "Human Detections Details: [{'bbox': [694, 423, 720, 444], 'confidence': 0.8897274136543274, 'center': [707, 434]}, {'bbox': [543, 61, 569, 80], 'confidence': 0.8840972185134888, 'center': [556, 70]}, {'bbox': [5, 623, 36, 640], 'confidence': 0.8533737659454346, 'center': [21, 631]}, {'bbox': [141, 595, 172, 614], 'confidence': 0.8300378918647766, 'center': [156, 604]}, {'bbox': [59, 633, 77, 673], 'confidence': 0.7921713590621948, 'center': [68, 653]}, {'bbox': [610, 668, 636, 682], 'confidence': 0.788067102432251, 'center': [623, 675]}, {'bbox': [370, 219, 389, 239], 'confidence': 0.7823704481124878, 'center': [379, 229]}, {'bbox': [168, 56, 198, 72], 'confidence': 0.7673643827438354, 'center': [183, 64]}, {'bbox': [241, 658, 265, 682], 'confidence': 0.7376306056976318, 'center': [253, 670]}, {'bbox': [623, 554, 645, 576], 'confidence': 0.7257211804389954, 'center': [634, 565]}, {'bbox': [69, 62, 105, 80], 'confidence': 0.7038997411727905, 'center': [87, 71]}, {'bbox': [236, 107, 246, 119], 'confidence': 0.6890013813972473, 'center': [241, 113]}, {'bbox': [101, 138, 124, 151], 'confidence': 0.6856268644332886, 'center': [112, 145]}, {'bbox': [441, 1, 454, 25], 'confidence': 0.6776764988899231, 'center': [447, 13]}, {'bbox': [604, 338, 625, 362], 'confidence': 0.670429527759552, 'center': [615, 350]}, {'bbox': [493, 27, 512, 33], 'confidence': 0.6274834871292114, 'center': [503, 30]}, {'bbox': [362, 359, 372, 373], 'confidence': 0.6118978261947632, 'center': [367, 366]}, {'bbox': [416, 173, 423, 187], 'confidence': 0.6093188524246216, 'center': [420, 180]}, {'bbox': [189, 645, 206, 653], 'confidence': 0.60594242811203, 'center': [198, 649]}, {'bbox': [383, 547, 413, 566], 'confidence': 0.6037838459014893, 'center': [398, 557]}, {'bbox': [231, 409, 253, 427], 'confidence': 0.5879855155944824, 'center': [242, 418]}, {'bbox': [313, 294, 340, 314], 'confidence': 0.5815693140029907, 'center': [327, 304]}, {'bbox': [282, 249, 297, 262], 'confidence': 0.5707286596298218, 'center': [289, 256]}, {'bbox': [171, 385, 184, 418], 'confidence': 0.5677807331085205, 'center': [178, 401]}, {'bbox': [715, 508, 722, 518], 'confidence': 0.5503724813461304, 'center': [718, 513]}, {'bbox': [406, 154, 417, 170], 'confidence': 0.538118302822113, 'center': [411, 162]}, {'bbox': [57, 348, 83, 365], 'confidence': 0.493106871843338, 'center': [70, 356]}, {'bbox': [274, 469, 292, 509], 'confidence': 0.47264620661735535, 'center': [283, 489]}, {'bbox': [589, 574, 597, 591], 'confidence': 0.4700474143028259, 'center': [593, 583]}, {'bbox': [165, 127, 178, 135], 'confidence': 0.4555794298648834, 'center': [172, 131]}, {'bbox': [692, 156, 707, 162], 'confidence': 0.45066753029823303, 'center': [700, 159]}, {'bbox': [276, 230, 280, 237], 'confidence': 0.449485719203949, 'center': [278, 233]}, {'bbox': [384, 551, 397, 566], 'confidence': 0.4475753605365753, 'center': [391, 559]}, {'bbox': [324, 294, 341, 313], 'confidence': 0.4418374001979828, 'center': [332, 303]}, {'bbox': [57, 349, 65, 364], 'confidence': 0.4177767038345337, 'center': [61, 357]}, {'bbox': [247, 449, 254, 462], 'confidence': 0.4131140410900116, 'center': [250, 455]}, {'bbox': [73, 347, 82, 360], 'confidence': 0.40882354974746704, 'center': [78, 353]}, {'bbox': [730, 324, 741, 333], 'confidence': 0.3822939693927765, 'center': [735, 328]}, {'bbox': [576, 108, 588, 117], 'confidence': 0.3796161413192749, 'center': [582, 112]}, {'bbox': [205, 106, 213, 115], 'confidence': 0.35898852348327637, 'center': [209, 110]}, {'bbox': [170, 277, 187, 330], 'confidence': 0.35811156034469604, 'center': [179, 304]}, {'bbox': [693, 155, 706, 161], 'confidence': 0.35527992248535156, 'center': [699, 158]}, {'bbox': [22, 264, 33, 273], 'confidence': 0.32537388801574707, 'center': [27, 268]}, {'bbox': [163, 618, 172, 633], 'confidence': 0.2505284249782562, 'center': [168, 625]}]\n",
      " Human Density Category: High Density\n",
      "Recommended Resources:\n",
      "   1. Major incident response team\n",
      "   2. Medical triage setup\n",
      "   3. Heavy recovery equipment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Image 3/5: collapsed_building_image0031_3.png ---\n",
      "\n",
      "--- Analyzing Scene: collapsed_building_image0031_3.png ---\n",
      " Classified Disaster Type: COLLAPSED_BUILDING (Confidence: 0.95)\n",
      "Humans Detected: 38\n",
      "Human Detections Details: [{'bbox': [132, 182, 155, 205], 'confidence': 0.926459789276123, 'center': [144, 194]}, {'bbox': [245, 165, 264, 195], 'confidence': 0.8956925868988037, 'center': [255, 180]}, {'bbox': [95, 49, 127, 66], 'confidence': 0.8953154683113098, 'center': [111, 58]}, {'bbox': [171, 245, 185, 277], 'confidence': 0.8900136351585388, 'center': [178, 261]}, {'bbox': [246, 266, 269, 291], 'confidence': 0.8705659508705139, 'center': [257, 278]}, {'bbox': [40, 278, 55, 291], 'confidence': 0.8316388726234436, 'center': [47, 285]}, {'bbox': [81, 12, 97, 42], 'confidence': 0.8266171216964722, 'center': [89, 27]}, {'bbox': [144, 236, 164, 249], 'confidence': 0.826376736164093, 'center': [154, 243]}, {'bbox': [40, 47, 67, 68], 'confidence': 0.8254494071006775, 'center': [54, 58]}, {'bbox': [56, 1, 89, 20], 'confidence': 0.8147081136703491, 'center': [72, 11]}, {'bbox': [252, 144, 265, 163], 'confidence': 0.7877148389816284, 'center': [259, 153]}, {'bbox': [268, 60, 281, 72], 'confidence': 0.7637250423431396, 'center': [274, 66]}, {'bbox': [142, 65, 149, 72], 'confidence': 0.7577036023139954, 'center': [145, 68]}, {'bbox': [101, 13, 128, 32], 'confidence': 0.7504271864891052, 'center': [114, 23]}, {'bbox': [92, 156, 120, 174], 'confidence': 0.7480515837669373, 'center': [106, 165]}, {'bbox': [69, 156, 80, 184], 'confidence': 0.7278944253921509, 'center': [75, 170]}, {'bbox': [48, 137, 55, 142], 'confidence': 0.7236138582229614, 'center': [52, 139]}, {'bbox': [113, 119, 118, 125], 'confidence': 0.7233509421348572, 'center': [115, 122]}, {'bbox': [249, 231, 257, 246], 'confidence': 0.7045379281044006, 'center': [253, 239]}, {'bbox': [38, 235, 48, 246], 'confidence': 0.7004866003990173, 'center': [43, 240]}, {'bbox': [120, 178, 124, 187], 'confidence': 0.690272331237793, 'center': [122, 183]}, {'bbox': [209, 77, 217, 83], 'confidence': 0.6826819181442261, 'center': [213, 80]}, {'bbox': [28, 88, 48, 98], 'confidence': 0.6825397610664368, 'center': [38, 93]}, {'bbox': [7, 214, 9, 220], 'confidence': 0.6460052132606506, 'center': [8, 217]}, {'bbox': [268, 128, 272, 136], 'confidence': 0.5840422511100769, 'center': [270, 132]}, {'bbox': [180, 297, 188, 302], 'confidence': 0.5767537355422974, 'center': [184, 300]}, {'bbox': [187, 229, 195, 240], 'confidence': 0.5760536193847656, 'center': [191, 234]}, {'bbox': [272, 287, 277, 295], 'confidence': 0.5699073076248169, 'center': [275, 291]}, {'bbox': [173, 19, 177, 25], 'confidence': 0.5651144981384277, 'center': [175, 22]}, {'bbox': [50, 17, 54, 25], 'confidence': 0.5536074638366699, 'center': [52, 21]}, {'bbox': [153, 300, 157, 305], 'confidence': 0.5425999164581299, 'center': [155, 302]}, {'bbox': [58, 172, 61, 178], 'confidence': 0.5244840383529663, 'center': [59, 175]}, {'bbox': [0, 194, 4, 198], 'confidence': 0.5061920881271362, 'center': [2, 196]}, {'bbox': [44, 156, 47, 162], 'confidence': 0.4618231952190399, 'center': [46, 159]}, {'bbox': [67, 60, 71, 69], 'confidence': 0.42765331268310547, 'center': [69, 65]}, {'bbox': [117, 14, 128, 32], 'confidence': 0.3952989876270294, 'center': [122, 23]}, {'bbox': [35, 88, 48, 98], 'confidence': 0.36029210686683655, 'center': [42, 93]}, {'bbox': [81, 2, 89, 21], 'confidence': 0.3434716463088989, 'center': [85, 12]}]\n",
      " Human Density Category: High Density\n",
      "Recommended Resources:\n",
      "   1. Multiple rescue teams (heavy urban search and rescue)\n",
      "   2. Medical triage setup\n",
      "   3. Temporary shelters\n",
      "   4. Food and water supplies\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Image 4/5: fire_image0416_1.png ---\n",
      "\n",
      "--- Analyzing Scene: fire_image0416_1.png ---\n",
      " Classified Disaster Type: FIRE (Confidence: 1.00)\n",
      "Humans Detected: 36\n",
      "Human Detections Details: [{'bbox': [31, 184, 53, 204], 'confidence': 0.9397034049034119, 'center': [42, 194]}, {'bbox': [5, 160, 30, 178], 'confidence': 0.924670934677124, 'center': [18, 169]}, {'bbox': [188, 104, 200, 130], 'confidence': 0.8979839086532593, 'center': [194, 117]}, {'bbox': [6, 44, 29, 65], 'confidence': 0.8849570751190186, 'center': [18, 54]}, {'bbox': [223, 228, 238, 246], 'confidence': 0.8822351098060608, 'center': [231, 237]}, {'bbox': [216, 120, 233, 132], 'confidence': 0.8091199994087219, 'center': [224, 126]}, {'bbox': [0, 95, 16, 112], 'confidence': 0.78221595287323, 'center': [8, 104]}, {'bbox': [183, 73, 198, 88], 'confidence': 0.7771599292755127, 'center': [191, 80]}, {'bbox': [6, 114, 23, 138], 'confidence': 0.7591658234596252, 'center': [15, 126]}, {'bbox': [16, 21, 31, 31], 'confidence': 0.7480382323265076, 'center': [24, 26]}, {'bbox': [180, 119, 187, 135], 'confidence': 0.7448732852935791, 'center': [184, 127]}, {'bbox': [52, 116, 59, 125], 'confidence': 0.7284666895866394, 'center': [55, 120]}, {'bbox': [44, 0, 61, 9], 'confidence': 0.7238140106201172, 'center': [52, 4]}, {'bbox': [152, 19, 167, 44], 'confidence': 0.7175399661064148, 'center': [160, 32]}, {'bbox': [203, 34, 218, 40], 'confidence': 0.6720273494720459, 'center': [211, 37]}, {'bbox': [92, 142, 112, 171], 'confidence': 0.6455851197242737, 'center': [102, 157]}, {'bbox': [29, 113, 47, 129], 'confidence': 0.6261503100395203, 'center': [38, 121]}, {'bbox': [181, 90, 187, 104], 'confidence': 0.6223405003547668, 'center': [184, 97]}, {'bbox': [145, 109, 148, 117], 'confidence': 0.6096856594085693, 'center': [146, 113]}, {'bbox': [137, 167, 140, 173], 'confidence': 0.5860738754272461, 'center': [139, 170]}, {'bbox': [238, 192, 249, 204], 'confidence': 0.544187605381012, 'center': [244, 198]}, {'bbox': [6, 114, 18, 138], 'confidence': 0.527042031288147, 'center': [12, 126]}, {'bbox': [79, 14, 86, 23], 'confidence': 0.525736391544342, 'center': [83, 18]}, {'bbox': [56, 235, 61, 242], 'confidence': 0.5044810771942139, 'center': [59, 239]}, {'bbox': [65, 59, 68, 63], 'confidence': 0.490568071603775, 'center': [66, 61]}, {'bbox': [238, 191, 243, 203], 'confidence': 0.4378405511379242, 'center': [240, 197]}, {'bbox': [37, 107, 40, 111], 'confidence': 0.4228423237800598, 'center': [38, 109]}, {'bbox': [65, 14, 86, 28], 'confidence': 0.42017170786857605, 'center': [76, 21]}, {'bbox': [100, 143, 112, 169], 'confidence': 0.40784886479377747, 'center': [106, 156]}, {'bbox': [65, 18, 75, 29], 'confidence': 0.3997068703174591, 'center': [70, 23]}, {'bbox': [92, 155, 100, 164], 'confidence': 0.3726702630519867, 'center': [96, 160]}, {'bbox': [96, 211, 98, 214], 'confidence': 0.35763034224510193, 'center': [97, 213]}, {'bbox': [215, 34, 218, 40], 'confidence': 0.33923667669296265, 'center': [217, 37]}, {'bbox': [181, 206, 184, 210], 'confidence': 0.3373127579689026, 'center': [182, 208]}, {'bbox': [164, 35, 168, 40], 'confidence': 0.305338978767395, 'center': [166, 38]}, {'bbox': [78, 14, 86, 26], 'confidence': 0.30417880415916443, 'center': [82, 20]}]\n",
      " Human Density Category: High Density\n",
      "Recommended Resources:\n",
      "   1. Multiple firefighting units (regional support)\n",
      "   2. Medical triage setup\n",
      "   3. Temporary shelters\n",
      "   4. Evacuation plans\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Image 5/5: fire_image0182_3.png ---\n",
      "\n",
      "--- Analyzing Scene: fire_image0182_3.png ---\n",
      " Classified Disaster Type: FIRE (Confidence: 1.00)\n",
      "Humans Detected: 30\n",
      "Human Detections Details: [{'bbox': [277, 347, 307, 365], 'confidence': 0.8965851068496704, 'center': [292, 356]}, {'bbox': [39, 362, 61, 385], 'confidence': 0.8939927816390991, 'center': [50, 374]}, {'bbox': [448, 339, 460, 363], 'confidence': 0.8641926050186157, 'center': [454, 351]}, {'bbox': [345, 73, 364, 98], 'confidence': 0.8518361449241638, 'center': [355, 86]}, {'bbox': [247, 328, 267, 348], 'confidence': 0.8465059995651245, 'center': [257, 338]}, {'bbox': [352, 34, 385, 51], 'confidence': 0.8145831823348999, 'center': [369, 43]}, {'bbox': [70, 252, 89, 283], 'confidence': 0.8052578568458557, 'center': [80, 268]}, {'bbox': [170, 141, 186, 156], 'confidence': 0.7983502149581909, 'center': [178, 148]}, {'bbox': [300, 220, 318, 243], 'confidence': 0.7940569519996643, 'center': [309, 231]}, {'bbox': [337, 10, 361, 33], 'confidence': 0.7883071303367615, 'center': [349, 21]}, {'bbox': [83, 110, 100, 120], 'confidence': 0.7727240324020386, 'center': [92, 115]}, {'bbox': [427, 53, 444, 74], 'confidence': 0.7535688877105713, 'center': [436, 64]}, {'bbox': [178, 329, 204, 347], 'confidence': 0.7467558979988098, 'center': [191, 338]}, {'bbox': [425, 212, 438, 234], 'confidence': 0.7224329710006714, 'center': [432, 223]}, {'bbox': [212, 224, 225, 254], 'confidence': 0.7188191413879395, 'center': [219, 239]}, {'bbox': [252, 389, 259, 406], 'confidence': 0.7116201519966125, 'center': [256, 397]}, {'bbox': [167, 195, 178, 209], 'confidence': 0.7027042508125305, 'center': [172, 202]}, {'bbox': [51, 304, 65, 321], 'confidence': 0.6980891823768616, 'center': [58, 312]}, {'bbox': [388, 88, 398, 104], 'confidence': 0.6865588426589966, 'center': [393, 96]}, {'bbox': [433, 393, 442, 406], 'confidence': 0.6819034218788147, 'center': [437, 399]}, {'bbox': [201, 357, 212, 373], 'confidence': 0.6573010683059692, 'center': [207, 365]}, {'bbox': [442, 294, 449, 301], 'confidence': 0.6214938163757324, 'center': [446, 297]}, {'bbox': [67, 212, 83, 221], 'confidence': 0.6145290732383728, 'center': [75, 216]}, {'bbox': [137, 208, 142, 216], 'confidence': 0.5820568799972534, 'center': [140, 212]}, {'bbox': [214, 328, 227, 338], 'confidence': 0.5071091651916504, 'center': [220, 333]}, {'bbox': [28, 268, 34, 276], 'confidence': 0.503246009349823, 'center': [31, 272]}, {'bbox': [192, 264, 200, 268], 'confidence': 0.4871324598789215, 'center': [196, 266]}, {'bbox': [178, 328, 196, 348], 'confidence': 0.43767470121383667, 'center': [187, 338]}, {'bbox': [212, 242, 226, 255], 'confidence': 0.3750750720500946, 'center': [219, 249]}, {'bbox': [109, 311, 115, 318], 'confidence': 0.32105329632759094, 'center': [112, 315]}]\n",
      " Human Density Category: High Density\n",
      "Recommended Resources:\n",
      "   1. Multiple firefighting units (regional support)\n",
      "   2. Medical triage setup\n",
      "   3. Temporary shelters\n",
      "   4. Evacuation plans\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt \n",
    "import cv2 \n",
    "import random \n",
    "\n",
    "def process_disaster_area(folder_path, max_images=None):\n",
    "\n",
    "    print(f\" PROCESSING DISASTER AREA: {folder_path}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    image_files = []\n",
    "    if os.path.exists(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            # Check for common image file extensions\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "                image_files.append(os.path.join(folder_path, file))\n",
    "        # image_files.sort() # Removed sorting for random selection\n",
    "\n",
    "    else:\n",
    "        print(f\" Error: Folder not found at {folder_path}\")\n",
    "        return []\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"⚠️ No image files found in {folder_path}\")\n",
    "        return []\n",
    "\n",
    "    # Shuffle the image files for random selection\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    # Limit the number of images if max_images is specified\n",
    "    if max_images is not None:\n",
    "        # Ensure max_images does not exceed the total number of images\n",
    "        max_images = min(max_images, len(image_files))\n",
    "        image_files = image_files[:max_images]\n",
    "        print(f\"Processing {len(image_files)} randomly selected images as requested.\")\n",
    "\n",
    "\n",
    "    # Process each image\n",
    "    analysis_reports = []\n",
    "    print(f\"Found {len(image_files)} images to process.\")\n",
    "    for i, img_path in enumerate(image_files):\n",
    "        print(f\"\\n--- Processing Image {i+1}/{len(image_files)}: {os.path.basename(img_path)} ---\")\n",
    "        try:\n",
    "            report = integrated_system.analyze_scene(img_path)\n",
    "            if report:\n",
    "                analysis_reports.append(report)\n",
    "                # Display the image after analysis\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    plt.figure(figsize=(6, 4)) \n",
    "                    plt.imshow(img_rgb)\n",
    "                    plt.title(f\"Processed: {os.path.basename(img_path)}\")\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                else:\n",
    "                    print(f\" Could not load image for display: {os.path.basename(img_path)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing image {os.path.basename(img_path)}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return analysis_reports\n",
    "\n",
    "sample_folder_path = r'D:/c2a_dataset/C2A_Dataset/new_dataset3/test/images'\n",
    "area_analysis_results = process_disaster_area(sample_folder_path, max_images=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b00b7aa5"
   },
   "source": [
    "# Task\n",
    "Deploy the trained YOLO human detection model and the Keras disaster type classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbb016a2"
   },
   "source": [
    "## Refine and save models\n",
    "\n",
    "### Subtask:\n",
    "Save the trained YOLO human detection model and the Keras disaster type classification model in a deployable format (e.g., TensorFlow SavedModel, ONNX, or a format suitable for a specific deployment platform).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1754418259044,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "ed6b58f2",
    "outputId": "0308e7eb-007b-4281-dc3d-5ca7ec6080a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO human detection model saved to: D:/model_yolo.pt\n",
      "Keras disaster classification model saved to: D:/resnet50v2_disaster_classification_savedmodel.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define paths to save the models\n",
    "yolo_save_path = r'D:/model_yolo.pt' # YOLOv8 native format\n",
    "classification_save_path = r'D:/resnet50v2_disaster_classification_savedmodel.h5' # TensorFlow SavedModel format\n",
    "\n",
    "# 1. Save the trained YOLO model\n",
    "try:\n",
    "    model.save(yolo_save_path)\n",
    "    print(f\"YOLO human detection model saved to: {yolo_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving YOLO model: {e}\")\n",
    "\n",
    "\n",
    "# 2. Save the trained Keras classification model in TensorFlow SavedModel format\n",
    "try:\n",
    "    model_classification.save(classification_save_path)\n",
    "    print(f\"Keras disaster classification model saved to: {classification_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Keras classification model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ffaedd8"
   },
   "source": [
    "## Deploy and test (Flask Application)\n",
    "\n",
    "### Subtask:\n",
    "Deploy the Flask application (e.g., using ngrok in Colab for testing, or to a cloud platform) and test it with new disaster images to ensure it functions correctly and provides timely recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "executionInfo": {
     "elapsed": 5842,
     "status": "error",
     "timestamp": 1754419217052,
     "user": {
      "displayName": "Akash Tyagi",
      "userId": "08565882076168294367"
     },
     "user_tz": -330
    },
    "id": "ae619482",
    "outputId": "3d6fec4f-442e-43a8-e2bb-79896462da9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated Disaster System Initialized\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.0.113:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:192.168.0.113 - - [08/Aug/2025 10:16:48] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:192.168.0.113 - - [08/Aug/2025 10:16:50] \"GET /upload HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Scene: tmpmy9ci2q4.png ---\n",
      "WARNING:tensorflow:5 out of the last 23 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023D1CC6AD40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 23 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023D1CC6AD40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INFO:werkzeug:192.168.0.113 - - [08/Aug/2025 10:17:02] \"POST /upload HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classified Disaster Type: COLLAPSED_BUILDING (Confidence: 1.00)\n",
      "Humans Detected: 17\n",
      " Human Density Category: High Density\n",
      "Recommended Resources:\n",
      "   1. Multiple rescue teams (heavy urban search and rescue)\n",
      "   2. Medical triage setup\n",
      "   3. Temporary shelters\n",
      "   4. Food and water supplies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:192.168.0.113 - - [08/Aug/2025 10:17:16] \"POST /upload HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Scene: tmpgu9bkua5.png ---\n",
      " Classified Disaster Type: COLLAPSED_BUILDING (Confidence: 0.99)\n",
      "Humans Detected: 40\n",
      " Human Density Category: High Density\n",
      "Recommended Resources:\n",
      "   1. Multiple rescue teams (heavy urban search and rescue)\n",
      "   2. Medical triage setup\n",
      "   3. Temporary shelters\n",
      "   4. Food and water supplies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:192.168.0.113 - - [08/Aug/2025 10:17:25] \"POST /upload HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from flask import Flask, request, jsonify, render_template_string\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_preprocess_input\n",
    "import base64\n",
    "\n",
    "# Rescue resource recommendations\n",
    "RESCUE_RESOURCES = {\n",
    "    'collapsed_building': {\n",
    "        'no_humans': ['Search and rescue dogs', 'Structural assessment team', 'Heavy equipment'],\n",
    "        'low_density': ['Ground rescue teams (small)', 'Medical aid (basic)', 'Search and rescue dogs'],\n",
    "        'medium_density': ['Ground rescue teams (multiple)', 'Medical aid (advanced)', 'Temporary shelter assessment'],\n",
    "        'high_density': ['Multiple rescue teams (heavy urban search and rescue)', 'Medical triage setup', 'Temporary shelters', 'Food and water supplies']\n",
    "    },\n",
    "    'flood': {\n",
    "        'no_humans': ['Flood damage assessment', 'Infrastructure check'],\n",
    "        'low_density': ['Water rescue team (small boat)', 'Medical aid (basic)', 'Evacuation route assessment'],\n",
    "        'medium_density': ['Water rescue teams (multiple boats)', 'Medical aid (advanced)', 'Evacuation support', 'Temporary shelters'],\n",
    "        'high_density': ['Multiple water rescue teams (boats/helicopters)', 'Medical triage setup', 'Temporary shelters', 'Food and water supplies', 'Evacuation plans']\n",
    "    },\n",
    "    'fire': {\n",
    "        'no_humans': ['Fire damage assessment', 'Safety perimeter setup'],\n",
    "        'low_density': ['Firefighting units (local)', 'Medical aid (basic)'],\n",
    "        'medium_density': ['Firefighting units (multiple)', 'Medical aid (advanced)', 'Evacuation route assessment'],\n",
    "        'high_density': ['Multiple firefighting units (regional support)', 'Medical triage setup', 'Temporary shelters', 'Evacuation plans']\n",
    "    },\n",
    "    'traffic_incident': {\n",
    "        'no_humans': ['Traffic management', 'Scene cleanup'],\n",
    "        'low_density': ['Police/Emergency medical (basic)', 'Tow trucks'],\n",
    "        'medium_density': ['Multiple police/emergency medical', 'Traffic management', 'Accident investigation'],\n",
    "        'high_density': ['Major incident response team', 'Medical triage setup', 'Heavy recovery equipment']\n",
    "    },\n",
    "    'other': {\n",
    "        'no_humans': ['General assessment team'],\n",
    "        'low_density': ['Basic rescue team', 'Medical aid (basic)'],\n",
    "        'medium_density': ['Multiple rescue teams', 'Medical aid (advanced)'],\n",
    "        'high_density': ['Major rescue operations', 'Medical triage setup']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Integrated Disaster System Class\n",
    "class IntegratedDisasterSystem:\n",
    "    def __init__(self, yolo_model, classification_model, classification_class_names):\n",
    "        self.yolo_model = yolo_model\n",
    "        self.classification_model = classification_model\n",
    "        self.classification_class_names = classification_class_names\n",
    "        print(\"Integrated Disaster System Initialized\")\n",
    "\n",
    "    def classify_disaster_type(self, image_path):\n",
    "        \"\"\"Classify the type of disaster in the image\"\"\"\n",
    "        img = keras_image.load_img(image_path, target_size=(self.classification_model.input_shape[1], self.classification_model.input_shape[2]))\n",
    "        img_array = keras_image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        # Assume resnet-style preprocessing\n",
    "        img_array = resnet_preprocess_input(img_array)\n",
    "        predictions = self.classification_model.predict(img_array, verbose=0)\n",
    "        predicted_class_index = np.argmax(predictions[0])\n",
    "        predicted_class_name = self.classification_class_names[predicted_class_index]\n",
    "        confidence = np.max(predictions[0])\n",
    "        return predicted_class_name, float(confidence)\n",
    "\n",
    "    def detect_humans(self, image_path):\n",
    "        \"\"\"Detect humans in disaster scene using YOLO model\"\"\"\n",
    "        results = self.yolo_model(image_path, conf=0.25, verbose=False)\n",
    "        detections = []\n",
    "        human_count = 0\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    # 'human' assumed class 0 in YOLO\n",
    "                    if box.cls[0] == 0:\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        conf = box.conf[0].cpu().numpy()\n",
    "                        detections.append({\n",
    "                            'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                            'confidence': float(conf),\n",
    "                            'center': [int((x1+x2)/2), int((y1+y2)/2)]\n",
    "                        })\n",
    "                        human_count += 1\n",
    "        return detections, human_count\n",
    "\n",
    "    def assess_human_density(self, human_count):\n",
    "        \"\"\"Assess human density category\"\"\"\n",
    "        if human_count == 0:\n",
    "            return 'no_humans'\n",
    "        elif human_count <= 2:\n",
    "            return 'low_density'\n",
    "        elif human_count <= 5:\n",
    "            return 'medium_density'\n",
    "        else:\n",
    "            return 'high_density'\n",
    "\n",
    "    def recommend_resources(self, disaster_type, human_density_category):\n",
    "        \"\"\"Recommend resources based on disaster type and human density\"\"\"\n",
    "        disaster_key = disaster_type.lower() if disaster_type.lower() in RESCUE_RESOURCES else 'other'\n",
    "        density_key = human_density_category.lower() if human_density_category.lower() in RESCUE_RESOURCES[disaster_key] else 'no_humans'\n",
    "        return RESCUE_RESOURCES[disaster_key][density_key]\n",
    "\n",
    "    def analyze_scene(self, image_path):\n",
    "        \"\"\"Analyze a single disaster scene and provide recommendations\"\"\"\n",
    "        print(f\"\\n--- Analyzing Scene: {os.path.basename(image_path)} ---\")\n",
    "        disaster_type, type_confidence = self.classify_disaster_type(image_path)\n",
    "        print(f\" Classified Disaster Type: {disaster_type.upper()} (Confidence: {type_confidence:.2f})\")\n",
    "        detections, human_count = self.detect_humans(image_path)\n",
    "        print(f\"Humans Detected: {human_count}\")\n",
    "        human_density_category = self.assess_human_density(human_count)\n",
    "        print(f\" Human Density Category: {human_density_category.replace('_', ' ').title()}\")\n",
    "        recommended_resources = self.recommend_resources(disaster_type, human_density_category)\n",
    "        print(\"Recommended Resources:\")\n",
    "        for i, resource in enumerate(recommended_resources, 1):\n",
    "            print(f\"   {i}. {resource}\")\n",
    "        image_base64 = None\n",
    "        try:\n",
    "            with open(image_path, \"rb\") as img_file:\n",
    "                image_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding image to base64: {e}\")\n",
    "        report = {\n",
    "            'image_path': image_path,\n",
    "            'disaster_classification': {'type': disaster_type, 'confidence': type_confidence},\n",
    "            'human_detection': {'count': human_count, 'detections': detections},\n",
    "            'human_density_category': human_density_category,\n",
    "            'recommended_resources': recommended_resources,\n",
    "            'image_base64': image_base64\n",
    "        }\n",
    "        return report\n",
    "\n",
    "# Model Loading Paths\n",
    "YOLO_MODEL_PATH = r\"D:/model_yolo.pt\"\n",
    "CLASSIFICATION_MODEL_PATH = r\"D:/resnet50v2_disaster_classification_savedmodel.h5\"\n",
    "\n",
    "# Load YOLO model\n",
    "try:\n",
    "    yolo_model = YOLO(YOLO_MODEL_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load YOLO model: {e}\")\n",
    "    yolo_model = None\n",
    "\n",
    "# Load classification model\n",
    "try:\n",
    "    classification_model = tf.keras.models.load_model(CLASSIFICATION_MODEL_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load classification model: {e}\")\n",
    "    classification_model = None\n",
    "\n",
    "disaster_types = ['collapsed_building', 'flood', 'fire', 'traffic_incident']\n",
    "\n",
    "if yolo_model and classification_model:\n",
    "    integrated_system = IntegratedDisasterSystem(yolo_model, classification_model, disaster_types)\n",
    "else:\n",
    "    integrated_system = None\n",
    "\n",
    "# Flask App Setup\n",
    "app = Flask(__name__)\n",
    "\n",
    "UPLOAD_HTML = \"\"\"\n",
    "<!doctype html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <title>Disaster Image Analysis</title>\n",
    "  <style>\n",
    "    body {\n",
    "      background: linear-gradient(135deg, #89f7fe 0%, #66a6ff 100%);\n",
    "      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "      color: #222;\n",
    "      margin: 40px auto;\n",
    "      max-width: 700px;\n",
    "      padding: 20px;\n",
    "      border-radius: 15px;\n",
    "      box-shadow: 0 8px 16px rgba(0,0,0,0.3);\n",
    "      background-color: white;\n",
    "    }\n",
    "    h1 {\n",
    "      color: #0056b3;\n",
    "      text-align: center;\n",
    "      margin-bottom: 30px;\n",
    "    }\n",
    "    form {\n",
    "      display: flex;\n",
    "      flex-direction: column;\n",
    "      align-items: center;\n",
    "      margin-bottom: 30px;\n",
    "    }\n",
    "    input[type=\"file\"] {\n",
    "      padding: 10px;\n",
    "      border: 2px dashed #0056b3;\n",
    "      border-radius: 10px;\n",
    "      width: 90%;\n",
    "      max-width: 350px;\n",
    "      margin-bottom: 15px;\n",
    "      cursor: pointer;\n",
    "      transition: border-color 0.3s ease;\n",
    "    }\n",
    "    input[type=\"file\"]:hover {\n",
    "      border-color: #003d75;\n",
    "    }\n",
    "    input[type=\"submit\"] {\n",
    "      background-color: #0056b3;\n",
    "      color: white;\n",
    "      font-weight: bold;\n",
    "      border: none;\n",
    "      border-radius: 10px;\n",
    "      padding: 12px 25px;\n",
    "      cursor: pointer;\n",
    "      transition: background-color 0.3s ease;\n",
    "    }\n",
    "    input[type=\"submit\"]:hover {\n",
    "      background-color: #003d75;\n",
    "    }\n",
    "    .result {\n",
    "      background-color: #f0f7ff;\n",
    "      border: 2px solid #66a6ff;\n",
    "      border-radius: 10px;\n",
    "      padding: 20px;\n",
    "      white-space: pre-wrap;\n",
    "      font-family: 'Courier New', Courier, monospace;\n",
    "      font-size: 1rem;\n",
    "      color: #003d75;\n",
    "    }\n",
    "    .result h2 {\n",
    "      color: #0056b3;\n",
    "      text-align: center;\n",
    "      margin-bottom: 20px;\n",
    "    }\n",
    "    .line {\n",
    "      margin-bottom: 10px;\n",
    "      padding-left: 10px;\n",
    "      border-left: 3px solid #0056b3;\n",
    "    }\n",
    "    img.uploaded-image {\n",
    "      max-width: 100%;\n",
    "      height: auto;\n",
    "      border-radius: 10px;\n",
    "      display: block;\n",
    "      margin: 20px auto;\n",
    "      box-shadow: 0 4px 12px rgba(0,0,0,0.15);\n",
    "    }\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>Upload an image for disaster analysis</h1>\n",
    "  <form method=\"post\" enctype=\"multipart/form-data\">\n",
    "    <input type=\"file\" name=\"image\" accept=\"image/png, image/jpeg\">\n",
    "    <input type=\"submit\" value=\"Analyze\">\n",
    "  </form>\n",
    "\n",
    "  {% if result_lines %}\n",
    "  <div class=\"result\">\n",
    "    <h2>Result</h2>\n",
    "    {% for line in result_lines %}\n",
    "      <div class=\"line\">{{ line }}</div>\n",
    "    {% endfor %}\n",
    "  </div>\n",
    "  {% endif %}\n",
    "\n",
    "  {% if image_base64 %}\n",
    "  <img class=\"uploaded-image\" src=\"data:image/png;base64,{{ image_base64 }}\" alt=\"Uploaded Image\" />\n",
    "  {% endif %}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def home():\n",
    "    return '<a href=\"/upload\">Go to interactive disaster analysis page</a>'\n",
    "\n",
    "@app.route('/upload', methods=['GET', 'POST'])\n",
    "def upload_page():\n",
    "    result_lines = None\n",
    "    image_base64 = None\n",
    "    temp_path = None\n",
    "    if request.method == 'POST':\n",
    "        if 'image' not in request.files or request.files['image'].filename == '':\n",
    "            result_lines = [\"No file selected!\"]\n",
    "        else:\n",
    "            file = request.files['image']\n",
    "            try:\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as temp:\n",
    "                    temp_path = temp.name\n",
    "                    file.save(temp_path)\n",
    "                if integrated_system:\n",
    "                    report = integrated_system.analyze_scene(temp_path)\n",
    "                    result_lines = [\n",
    "                        f\"Disaster Type: {report['disaster_classification']['type']} (Confidence: {report['disaster_classification']['confidence']:.2f})\",\n",
    "                        f\"Human Count: {report['human_detection']['count']}\",\n",
    "                        f\"Human Density Category: {report['human_density_category'].replace('_', ' ').title()}\",\n",
    "                        \"Resource Recommendations:\"\n",
    "                    ] + [f\"  - {res}\" for res in report['recommended_resources']]\n",
    "                    image_base64 = report.get('image_base64', None)\n",
    "                else:\n",
    "                    result_lines = [\"System not initialized.\"]\n",
    "            except Exception as ex:\n",
    "                result_lines = [f\"Analysis failed. {ex}\"]\n",
    "            finally:\n",
    "                if temp_path and os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "    return render_template_string(UPLOAD_HTML, result_lines=result_lines, image_base64=image_base64)\n",
    "\n",
    "@app.route('/analyze', methods=['POST'])\n",
    "def analyze_image():\n",
    "    temp_path = None\n",
    "    if integrated_system is None:\n",
    "        return jsonify({\"error\": \"System not initialized properly.\"}), 500\n",
    "    if 'image' not in request.files:\n",
    "        return jsonify({\"error\": \"No image file provided.\"}), 400\n",
    "    file = request.files['image']\n",
    "    if file.filename == '':\n",
    "        return jsonify({\"error\": \"No selected file.\"}), 400\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as temp:\n",
    "            temp_path = temp.name\n",
    "            file.save(temp_path)\n",
    "        report = integrated_system.analyze_scene(temp_path)\n",
    "        return jsonify(report), 200\n",
    "    except Exception as ex:\n",
    "        return jsonify({\"error\": \"Analysis failed.\", \"details\": str(ex)}), 500\n",
    "    finally:\n",
    "        if temp_path and os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOFk35FDBhEchbH/LZT3BLu",
   "collapsed_sections": [
    "05cc24a7",
    "094232fb",
    "4e6d48e8",
    "2a68183c",
    "4f2321e2"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
